INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 16] at entry 0 and [1, 186, 192, 16] at entry 4

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 58)
Value range: (-2048.0, 1775.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 73)
Value range: (-3775.0, 20594.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 68)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20594, dtype=int32), 'glmin': array(-3775, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-342.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0046_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  68,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.827, 0.827, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(217.359, dtype=float32), 'qoffset_y': array(211.5001, dtype=float32), 'qoffset_z': array(-940., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  68], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0044_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 68)
Value range: (-2204.0, 5501.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 286)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  68,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.827, 0.827, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(5501, dtype=int32), 'glmin': array(-2204, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(217.359, dtype=float32), 'qoffset_y': array(211.5001, dtype=float32), 'qoffset_z': array(-940., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  68], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0044_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512, 286,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 1.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(184.785, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-940.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512, 286], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0014_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 65)
Value range: (-1024.0, 1471.0)
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 77)
Value range: (-2048.0, 3413.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 65)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  65,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.      , 0.816406, 0.816406, 5.      , 0.      , 0.      ,
       0.      , 0.      ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1471, dtype=int32), 'glmin': array(-1024, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(233.1, dtype=float32), 'qoffset_y': array(209., dtype=float32), 'qoffset_z': array(-361.3, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.81640601,    0.        ,    0.        ,  233.1000061 ],
       [   0.        ,   -0.81640601,    0.        ,  209.        ],
       [   0.        ,    0.        ,    5.        , -361.29998779],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.81640601,    0.        ,    0.        ,  233.1000061 ],
       [   0.        ,   -0.81640601,    0.        ,  209.        ],
       [   0.        ,    0.        ,    5.        , -361.29998779],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  65], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0077_ct.nii.gz'}
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  65,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.      , 0.816406, 0.816406, 5.      , 0.      , 0.      ,
       0.      , 0.      ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(233.1, dtype=float32), 'qoffset_y': array(209., dtype=float32), 'qoffset_z': array(-361.3, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.81640601,    0.        ,    0.        ,  233.1000061 ],
       [   0.        ,   -0.81640601,    0.        ,  209.        ],
       [   0.        ,    0.        ,    5.        , -361.29998779],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.81640601,    0.        ,    0.        ,  233.1000061 ],
       [   0.        ,   -0.81640601,    0.        ,  209.        ],
       [   0.        ,    0.        ,    5.        , -361.29998779],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  65], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0077_seg.nii.gz'}
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 63)
Value range: (-4334.0, 25789.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 63)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.751, 0.751, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(25789, dtype=int32), 'glmin': array(-4334, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(179.311, dtype=float32), 'qoffset_y': array(192.007, dtype=float32), 'qoffset_z': array(-238.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.75099999,    0.        ,    0.        ,  179.31100464],
       [   0.        ,   -0.75099999,    0.        ,  192.00700378],
       [   0.        ,    0.        ,    5.        , -238.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.75099999,    0.        ,    0.        ,  179.31100464],
       [   0.        ,   -0.75099999,    0.        ,  192.00700378],
       [   0.        ,    0.        ,    5.        , -238.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0083_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.751, 0.751, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(179.311, dtype=float32), 'qoffset_y': array(192.007, dtype=float32), 'qoffset_z': array(-238.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.75099999,    0.        ,    0.        ,  179.31100464],
       [   0.        ,   -0.75099999,    0.        ,  192.00700378],
       [   0.        ,    0.        ,    5.        , -238.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.75099999,    0.        ,    0.        ,  179.31100464],
       [   0.        ,   -0.75099999,    0.        ,  192.00700378],
       [   0.        ,    0.        ,    5.        , -238.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0083_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 58)
Value range: (-6822.0, 20157.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 70)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  70,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(191.816, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-999.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.91815994e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.99500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.91815994e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.99500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  70], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0120_seg.nii.gz'}
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 70)
Value range: (-9653.0, 27503.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  70,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(27503, dtype=int32), 'glmin': array(-9653, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(191.816, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-999.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.91815994e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.99500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.91815994e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.99500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  70], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0120_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(189.472, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-909.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.89472000e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.09500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.89472000e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.09500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0110_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 70)
Value range: (-2048.0, 2083.0)
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 54)
Value range: (-2048.0, 8565.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 58)
Value range: (0.0, 1.0)
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 53)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  70,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.839, 0.839, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(2083, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(225.166, dtype=float32), 'qoffset_y': array(214.4241, dtype=float32), 'qoffset_z': array(-1070., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.38999987e-01,  0.00000000e+00,  0.00000000e+00,
         2.25166000e+02],
       [ 0.00000000e+00, -8.38999987e-01,  0.00000000e+00,
         2.14424103e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.07000000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.38999987e-01,  0.00000000e+00,  0.00000000e+00,
         2.25166000e+02],
       [ 0.00000000e+00, -8.38999987e-01,  0.00000000e+00,
         2.14424103e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.07000000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  70], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0154_ct.nii.gz'}
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(8565, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_ct.nii.gz'}
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 53)
Value range: (-3550.0, 10943.0)
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  53,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.781, 0.781, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(199.609, dtype=float32), 'qoffset_y': array(199.6093, dtype=float32), 'qoffset_z': array(-334., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.78100002,    0.        ,    0.        ,  199.60899353],
       [   0.        ,   -0.78100002,    0.        ,  199.60929871],
       [   0.        ,    0.        ,    5.        , -334.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.78100002,    0.        ,    0.        ,  199.60899353],
       [   0.        ,   -0.78100002,    0.        ,  199.60929871],
       [   0.        ,    0.        ,    5.        , -334.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  53], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0179_seg.nii.gz'}
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 286)
Value range: (-3569.0, 14746.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 70)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  53,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.781, 0.781, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(10943, dtype=int32), 'glmin': array(-3550, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(199.609, dtype=float32), 'qoffset_y': array(199.6093, dtype=float32), 'qoffset_z': array(-334., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -0.78100002,    0.        ,    0.        ,  199.60899353],
       [   0.        ,   -0.78100002,    0.        ,  199.60929871],
       [   0.        ,    0.        ,    5.        , -334.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.78100002,    0.        ,    0.        ,  199.60899353],
       [   0.        ,   -0.78100002,    0.        ,  199.60929871],
       [   0.        ,    0.        ,    5.        , -334.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  53], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0179_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  70,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.839, 0.839, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(225.166, dtype=float32), 'qoffset_y': array(214.4241, dtype=float32), 'qoffset_z': array(-1070., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-8.38999987e-01,  0.00000000e+00,  0.00000000e+00,
         2.25166000e+02],
       [ 0.00000000e+00, -8.38999987e-01,  0.00000000e+00,
         2.14424103e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.07000000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.38999987e-01,  0.00000000e+00,  0.00000000e+00,
         2.25166000e+02],
       [ 0.00000000e+00, -8.38999987e-01,  0.00000000e+00,
         2.14424103e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.07000000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  70], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0154_seg.nii.gz'}
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 291)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512, 286,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 1.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(14746, dtype=int32), 'glmin': array(-3569, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(184.785, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-940.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512, 286], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0014_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512, 291,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 1.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(199.825, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-910.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         1.99824997e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.10500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         1.99824997e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.10500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512, 291], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0003_seg.nii.gz'}
INFO:DataStats:
=== Transform input info -- Trace ===
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 512, 512, 68)
Value range: (-2048.0, 1668.0)
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/17 -- loss: 1.5317 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/17 -- loss: 1.5234 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/17 -- loss: 1.4795 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 4/17 -- loss: 1.4944 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 5/17 -- loss: 1.4626 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 6/17 -- loss: 1.4652 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 7/17 -- loss: 1.4374 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 8/17 -- loss: 1.4250 
ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: 
ERROR:ignite.engine.engine.SupervisedTrainer:Exception: 
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 833, in _run_once_on_dataset
    self.state.output = self._process_function(self, self.state.batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/engines/trainer.py", line 175, in _iteration
    self.scaler.step(self.optimizer)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140002205738896, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140002205738704, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140002205739344, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140002205738896, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140002205738704, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140002205739344, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140002205738896, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140002205738704, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140002205739344, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140002205738896, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140002205738704, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140002205739344, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 298, in __call__
    SLICE = d.get('label_transforms')[3].get('extra_info').get('center')[-1]
IndexError: list index out of range

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f54cdbdab10>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (364, 364, 58), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140154526796816, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140154526794960, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140154526797328, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140154526797456, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (304, 304, 73), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140154526796816, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140154526794960, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140154526797328, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140154526797456, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.000000238418579)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140154526796816, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140154526794448, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140154526795728, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140154526796368, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140154526796112, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
TypeError: list indices must be integers or slices, not tuple

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7844c88bd0>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967120967504, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139967120966352, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967120968528, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967120968208, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967120967504, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139967120966352, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967120968528, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967120968208, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967120967504, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139967120966352, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967120968528, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967120968208, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967120967504, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139967120966352, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967120968528, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967120968208, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f4ca2868950>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140540487454160, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140540487454352, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140540487451792, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140540487454224, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140540487454160, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140540487454352, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140540487451792, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140540487454224, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140540487454160, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140540487454352, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140540487451792, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140540487454224, 'orig_size': (364, 364, 58), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140540484804688, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140540484805008, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140540484805328, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140540484805456, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140540487454160, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140540487454352, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140540487451792, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140540487454224, 'orig_size': (304, 304, 73), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140540484804688, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140540484805008, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140540484805328, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140540484805456, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.000000238418579)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140540484804688, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/17 -- loss: 1.5317 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/17 -- loss: 1.5234 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/17 -- loss: 1.4795 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 4/17 -- loss: 1.4944 
ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: 
ERROR:ignite.engine.engine.SupervisedTrainer:Exception: 
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 833, in _run_once_on_dataset
    self.state.output = self._process_function(self, self.state.batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/engines/trainer.py", line 175, in _iteration
    self.scaler.step(self.optimizer)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (364, 364, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140331833603600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140331833604240, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140331833604496, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140331833604624, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (304, 304, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140331833603600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140331833604240, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140331833604496, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140331833604624, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140331833603600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140331833603280, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140331833603472, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140331833603088, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140331833603536, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa18d17b650>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (364, 364, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140117342476432, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140117342476176, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140117342476752, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140117342476816, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (304, 304, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140117342476432, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140117342476176, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140117342476752, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140117342476816, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140117342476432, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140117342475792, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140117342475984, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140117342476304, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140117342476496, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f6f9c6cd490>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140194767708624, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140194767708816, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140194767709200, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140194767708944, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140194767708624, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140194767708816, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140194767709200, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140194767708944, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140194767708624, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140194767708816, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140194767709200, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140194767708944, 'orig_size': (364, 364, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140194767709008, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140194767709520, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140194767709904, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140194767710032, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140194767708624, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140194767708816, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140194767709200, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140194767708944, 'orig_size': (304, 304, 73), 'extra_info': {'center': [104, 181, 39]}}, {'class': 'RandAffined', 'id': 140194767709008, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140194767709520, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140194767709904, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140194767710032, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140194767709008, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139629381849872, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139629381852240, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139629381852560, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139629381852304, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139629381849872, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139629381852240, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139629381852560, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139629381852304, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139629381849872, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139629381852240, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139629381852560, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139629381852304, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139629381849872, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139629381852240, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139629381852560, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139629381852304, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7efdffb49810>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139660894671568, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139660894671376, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139660894673104, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139660894673232, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139660894671568, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139660894671376, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139660894673104, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139660894673232, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139660894671568, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139660894671376, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139660894673104, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139660894673232, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139660894671568, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139660894671376, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139660894673104, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139660894673232, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 318, in __call__
    slice_healthy_inpain = pseudo_healthy_with_texture(scan_slice, lesions_all, coords_all, masks_all, names_all, self.texture)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 131, in pseudo_healthy_with_texture
    slice_healthy[new_coords_mask] = texture[new_coords_mask]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0556041050>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0000001192092896)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (364, 364, 58), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465296, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465552, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465680, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (304, 304, 73), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465296, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465552, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465680, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.000000238418579)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.02853776328265667, 1.0262305736541748)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 141, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465296, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465552, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465680, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140716220405328, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140716220406864, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140716220407248, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140716220406992, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465296, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140716220465552, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140716220465680, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.008814029395580292, 0.007789191324263811)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140716220406544, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 414, in __call__
    if np.random.rand() > self.replace_image_for_synthetic:
TypeError: '>' not supported between instances of 'float' and 'tuple'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7ffb0c55c290>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139637277178384, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139637277177552, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139637274493328, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139637274493456, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139637277178384, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139637277177552, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139637274493328, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139637274493456, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140687932105232, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140687932107152, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140687932107472, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140687932107216, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140687932105232, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140687932107152, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140687932107472, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140687932107216, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140488790883600, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140488790883792, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140488790883664, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140488790950032, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140488790883600, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140488790883792, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140488790883664, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140488790950032, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139801304883984, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139801304884304, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139801304884688, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139801304884816, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139801304883984, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139801304884304, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139801304884688, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139801304884816, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140592235920464, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140592235920656, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140592235920528, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140592235921296, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140592235920464, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140592235920656, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140592235920528, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140592235921296, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139905063987472, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139905063986000, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139905063989200, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139905063988880, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139905063987472, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139905063986000, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139905063989200, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139905063988880, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140454828278480, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140454828280144, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140454828280464, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140454828280208, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140454828278480, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140454828280144, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140454828280464, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140454828280208, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 141, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 174, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140295427644816, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140295427645008, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140295577268048, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140295427711120, 'orig_size': (304, 304, 68), 'extra_info': {'center': [268, 226, 46]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140295427644816, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140295427645008, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140295577268048, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140295427711120, 'orig_size': (364, 364, 68), 'extra_info': {'center': [268, 226, 46]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140295427644816, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140295427645008, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140295577268048, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140295427711120, 'orig_size': (355, 355, 77), 'extra_info': {'center': [208, 104, 27]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140295427644816, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140295427645008, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140295577268048, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140295427711120, 'orig_size': (304, 304, 68), 'extra_info': {'center': [208, 104, 27]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 335, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 58 is out of bounds for axis 3 with size 58

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f991320d8d0>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.000000238418579)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (364, 364, 58), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087120, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087376, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087504, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'SpatialPadd', 'id': 140230000087696, 'orig_size': (192, 192, 16), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (304, 304, 73), 'extra_info': {'center': [109, 194, 41]}}, {'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087120, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087376, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087504, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.000000238418579)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[ 1.0695e+00,  0.0000e+00,  4.8946e-02,  0.0000e+00],
        [ 1.2640e-04,  1.0306e+00, -2.4089e-03,  0.0000e+00],
        [-5.2409e-02,  2.4855e-03,  9.9880e-01,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],
       dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': True}]
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.02853776328265667, 1.0262305736541748)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 141, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087120, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087376, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087504, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'SpatialPadd', 'id': 140230000087696, 'orig_size': (192, 141, 16), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140230000028624, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140230000028752, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140230000029008, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140230000029136, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087120, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140230000087376, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140230000087504, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 141, 192, 16)
Value range: (-0.008814029395580292, 0.007789191324263811)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140230000027600, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/dictionary.py", line 126, in __call__
    d[key] = self.padder(d[key], mode=m)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 97, in __call__
    data_pad_width = self._determine_data_pad_width(img.shape[1:])
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/array.py", line 78, in _determine_data_pad_width
    self.spatial_size = fall_back_tuple(self.spatial_size, data_shape)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 174, in fall_back_tuple
    user = ensure_tuple_rep(user_provided, ndim)
  File "/usr/local/lib/python3.7/dist-packages/monai/utils/misc.py", line 131, in ensure_tuple_rep
    raise ValueError(f"Sequence must have length {dim}, got {len(tup)}.")
ValueError: Sequence must have length 3, got 4.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <monai.transforms.croppad.dictionary.SpatialPadd object at 0x7f89d7579290>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139754034296144, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139754034297936, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139754034298256, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139754034298000, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139754034296144, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139754034297936, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139754034298256, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139754034298000, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139802093035984, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139802093037456, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139802093038096, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139802093037840, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139802093035984, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139802093037456, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139802093038096, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139802093037840, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139730498660880, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139730498661008, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139730498661200, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139730498661840, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139730498660880, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139730498661008, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139730498661200, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139730498661840, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140224244820688, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140224244820496, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140224244820880, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140224244820816, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140224244820688, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140224244820496, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140224244820880, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140224244820816, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 380, in pad_list_data_collate
    return PadListDataCollate(method, mode)(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/croppad/batch.py", line 113, in __call__
    return list_data_collate(batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 744, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 848, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 148, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 274, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 285, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 192, 192, 5] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140581283959440, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140581283959248, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140581283959568, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140581283960336, 'orig_size': (304, 304, 63), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140581283959440, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140581283959248, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140581283959568, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140581283960336, 'orig_size': (364, 364, 58), 'extra_info': {'center': [208, 114, 19]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 132, 132, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140087636564816, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140087636565008, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140087636564944, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140087636565584, 'orig_size': (304, 304, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140087636564816, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140087636565008, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140087636564944, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140087636565584, 'orig_size': (364, 364, 73), 'extra_info': {'center': [268, 268, 13]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 355, 355, 77] at entry 0 and [1, 304, 304, 58] at entry 3

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz'}
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz'}
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20594, dtype=int32), 'glmin': array(-3775, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-342.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0046_ct.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_seg.nii.gz'}
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -342.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 339, 339, 68)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 339, 339, 68)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  68,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.827, 0.827, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(5501, dtype=int32), 'glmin': array(-2204, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(217.359, dtype=float32), 'qoffset_y': array(211.5001, dtype=float32), 'qoffset_z': array(-940., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  217.35899353],
       [   0.        ,   -1.25      ,    0.        ,  211.50010681],
       [   0.        ,    0.        ,    5.        , -940.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  68], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0044_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  68,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.827, 0.827, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(217.359, dtype=float32), 'qoffset_y': array(211.5001, dtype=float32), 'qoffset_z': array(-940., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  217.35899353],
       [   0.        ,   -1.25      ,    0.        ,  211.50010681],
       [   0.        ,    0.        ,    5.        , -940.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  68], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0044_seg.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 68), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 68), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (339, 339, 68), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 68), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 68), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.27000022e-01,  0.00000000e+00,  0.00000000e+00,
         2.17358994e+02],
       [ 0.00000000e+00, -8.27000022e-01,  0.00000000e+00,
         2.11500107e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.40000000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 339, 339, 68)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 339, 339, 68)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_seg.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(8565, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_seg.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512, 286,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 1.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(14746, dtype=int32), 'glmin': array(-3569, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(184.785, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-940.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  184.78500366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -940.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512, 286], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0014_ct.nii.gz'}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512, 286,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 1.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(184.785, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-940.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  184.78500366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -940.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512, 286], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0014_seg.nii.gz'}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 286), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 286), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139736290884688, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139736290884240, 'orig_size': (512, 512, 286), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139736290884304, 'orig_size': (512, 512, 286), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.84785004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,
        -9.40500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.02853776328265667, 1.0262305736541748)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140164913282448, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140164913282256, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140164913348752, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140164913348944, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140164913282128, 'orig_size': (192, 141, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140164913349264, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140164913349520, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140164913349648, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140164913282448, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140164913282256, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140164913348752, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140164913348944, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140164913282128, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140164913349264, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140164913349520, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140164913349648, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.008814029395580292, 0.007789191324263811)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140164913282128, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f7aafdec490>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.02853776328265667, 1.0262305736541748)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140008934575696, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140008934575824, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140008934646224, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140008934645904, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140008934575440, 'orig_size': (192, 141, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140008934646416, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140008934646800, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140008934646928, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140008934575696, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140008934575824, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140008934646224, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140008934645904, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 140008934575440, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140008934646416, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140008934646800, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140008934646928, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.008814029395580292, 0.007789191324263811)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140008934575440, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = pad_because_monai_transf_is_not_doing_it(array_trans)
NameError: name 'pad_because_monai_transf_is_not_doing_it' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f565ed0a510>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.02853776328265667, 1.0262305736541748)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139918469250256, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139918469250064, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139918469250384, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139918469316752, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 139918469249936, 'orig_size': (192, 141, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139918469317072, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139918469317328, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139918469317456, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139918469250256, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139918469250064, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139918469250384, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139918469316752, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}, {'class': 'RandAffined', 'id': 139918469249936, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139918469317072, 'orig_size': (192, 141, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139918469317328, 'orig_size': (192, 141, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139918469317456, 'orig_size': (192, 141, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (-0.008814029395580292, 0.007789191324263811)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 139918469249936, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 432, in __call__
    array_trans = self.pad_because_monai_transf_is_not_doing_it(array_trans)
  File "run_net_aug.py", line 416, in pad_because_monai_transf_is_not_doing_it
    array = np.pad(array, ((0,0),(0,ss-shy),(0,ss-shx),(0,0)),mode='reflect')
NameError: name 'ss' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f414ea943d0>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 186, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140204707269840, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140204707269968, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140204707270032, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140204707344464, 'orig_size': (304, 304, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140204707269840, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140204707269968, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140204707270032, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140204707344464, 'orig_size': (310, 310, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5036 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4450 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4108 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f83f3c5c890>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 186, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140127860071824, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140127860071952, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140127860142160, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140127860142288, 'orig_size': (304, 304, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140127860071824, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140127860071952, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140127860142160, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140127860142288, 'orig_size': (310, 310, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5043 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4458 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4139 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I-1]) # XX remove -1
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f720f529950>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 186, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (304, 304, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (310, 310, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5036 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4450 
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (364, 364, 58), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 2), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966238416, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966240912, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140186963045072, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966238416, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966240912, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140186963045072, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4108 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/run_net_aug.py", line 338, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f7fd22267d0>

INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(8565, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (310, 310, 58), 'extra_info': {'center': [96, 152, 8]}}, {'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966238416, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966240912, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186963045072, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140186963043600, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140186963044112, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140186963044880, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140186963044496, 'orig_size': (304, 304, 63), 'extra_info': {'center': [96, 152, 8]}}, {'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966238416, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186966240912, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140186963045072, 'orig_size': (192, 192, 16), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140186963044560, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5036 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4450 
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140498931377936, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140498931377360, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140498931378832, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140498931378960, 'orig_size': (364, 364, 58), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140498931378512, 'orig_size': (192, 192, 2), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140498934572880, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140498934573712, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140498931379024, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140498931377936, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140498931377360, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140498931378832, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140498931378960, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140498931378512, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140498934572880, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140498934573712, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140498931379024, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140498931378512, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4108 
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140568421829776, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140568421829584, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140568421829904, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140568421830608, 'orig_size': (355, 355, 77), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140568421829776, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140568421829584, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140568421829904, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140568421830608, 'orig_size': (304, 304, 73), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140568421829776, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140568421829584, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140568421829904, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140568421830608, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140568421829776, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140568421829584, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140568421829904, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140568421830608, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd8a2d98690>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140544658434832, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140544658434640, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140544658435024, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140544658434960, 'orig_size': (355, 355, 77), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140544658434832, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140544658434640, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140544658435024, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140544658434960, 'orig_size': (304, 304, 73), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140544658434832, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140544658434640, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140544658435024, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140544658434960, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140544658434832, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140544658434640, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140544658435024, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140544658434960, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 392, in __call__
    scan_slices = np.concatenate((scan_slices, crop_and_pad(scan_slice, CENTER_Y, CENTER_X)), 0)
  File "/content/drive/MyDrive/repositories/covid19_monai_segmentation/utils_replace_lesions.py", line 236, in crop_and_pad
    sh_y, sh_x = np.shape(scan_slice)
ValueError: not enough values to unpack (expected 2, got 1)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd31a70e210>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140327575530320, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140327575530448, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140327575530832, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140327575530576, 'orig_size': (355, 355, 77), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140327575530320, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140327575530448, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140327575530832, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140327575530576, 'orig_size': (304, 304, 73), 'extra_info': {'center': [196, 96, 46]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140327575530320, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140327575530448, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140327575530832, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140327575530576, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140327575530320, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140327575530448, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140327575530832, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140327575530576, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 340, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fa08f4aa710>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(3413, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140690877711120, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140690877713616, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140690877714000, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140690877713744, 'orig_size': (355, 355, 77), 'extra_info': {'center': [208, 96, 8]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140690877711120, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140690877713616, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140690877714000, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140690877713744, 'orig_size': (304, 304, 58), 'extra_info': {'center': [208, 96, 8]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 58)
Value range: (0.0, 1.0)
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 141, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1775, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(187.91, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-918.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  187.91000366],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -918.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0053_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  77,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.866, 0.866, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(221.246, dtype=float32), 'qoffset_y': array(221.2467, dtype=float32), 'qoffset_z': array(-1006.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[-1.25000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -1.25000000e+00,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'original_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  77], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0072_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140690877711120, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140690877713616, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40999997e-01,  0.00000000e+00,  0.00000000e+00,
         1.87910004e+02],
       [ 0.00000000e+00, -7.40999997e-01,  0.00000000e+00,
         1.89472900e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.18500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140690877714000, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140690877713744, 'orig_size': (304, 304, 58), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140690877711120, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140690877713616, 'orig_size': (512, 512, 77), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-8.65999997e-01,  0.00000000e+00,  0.00000000e+00,
         2.21246002e+02],
       [ 0.00000000e+00, -8.65999997e-01,  0.00000000e+00,
         2.21246704e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -1.00650000e+03],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140690877714000, 'orig_size': (355, 355, 77), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140690877713744, 'orig_size': (355, 355, 77), 'extra_info': {'center': [96, 259, 11]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 355, 355, 77)
Value range: (0.0, 1.0)
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 341, in __call__
    img1 = np.pad(img1,((0,100),(0,100),(0,20)))
  File "<__array_function__ internals>", line 6, in pad
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 746, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraypad.py", line 521, in _as_pairs
    return np.broadcast_to(x, (ndim, 2)).tolist()
  File "<__array_function__ internals>", line 6, in broadcast_to
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 180, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File "/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py", line 125, in _broadcast_to
    op_flags=['readonly'], itershape=shape, order='C')
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (4,2)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7ff525ca0810>

INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 186, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz', 'patch_index': 0}
INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 189, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1844, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139638135914000, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139638135914192, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139638135976144, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139638135976400, 'orig_size': (304, 304, 73), 'extra_info': {'center': [126, 211, 65]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139638135914000, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139638135914192, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139638135976144, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139638135976400, 'orig_size': (364, 364, 73), 'extra_info': {'center': [126, 211, 65]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 364, 364, 52)
Value range: (0.0, 1.0)
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5052 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.5316 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 397, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 57 is out of bounds for axis 3 with size 52

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7f0009cc7050>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5036 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4450 
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967635681040, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139967635680848, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967635755216, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967635755728, 'orig_size': (364, 364, 58), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 2), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756048, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756304, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139967635756432, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967635681040, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139967635680848, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967635755216, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967635755728, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756048, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756304, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139967635756432, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4108 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 4/9 -- train_loss: 1.4799 
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(8565, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  63,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(160.762, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-428.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  63], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0215_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967635681040, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 139967635680848, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967635755216, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967635755728, 'orig_size': (310, 310, 58), 'extra_info': {'center': [96, 152, 8]}}, {'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756048, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139967635756304, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756432, 'orig_size': (192, 192, 16), 'do_transforms': False}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 139967635681040, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 139967635680848, 'orig_size': (512, 512, 63), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.74000001,    0.        ,    0.        ,  160.76199341],
       [   0.        ,   -0.74000001,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -428.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 139967635755216, 'orig_size': (304, 304, 63), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 139967635755728, 'orig_size': (304, 304, 63), 'extra_info': {'center': [96, 152, 8]}}, {'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756048, 'orig_size': (192, 192, 16), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 139967635756304, 'orig_size': (192, 192, 16), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 139967635756432, 'orig_size': (192, 192, 16), 'do_transforms': False}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 63)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 139967635680912, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 5/9 -- train_loss: 1.4769 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 6/9 -- train_loss: 1.4135 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 436, in __call__
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
  File "run_net_aug.py", line 436, in <listcomp>
    aa = [np.shape(array_trans[0,...,i]) for i in range(16)]
IndexError: index 2 is out of bounds for axis 3 with size 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom2 object at 0x7f4cc1359610>

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 266, in list_data_collate
    return default_collate(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 74, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/utils.py", line 277, in list_data_collate
    raise RuntimeError(re_str)
RuntimeError: stack expects each tensor to be equal size, but got [1, 141, 192, 16] at entry 0 and [1, 192, 192, 16] at entry 1

MONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).

INFO:root:training: image/label (10) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 9 val 1, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 1
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 186, 192, 16)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  58,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.  , 0.74, 0.74, 5.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(20157, dtype=int32), 'glmin': array(-6822, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(206.661, dtype=float32), 'qoffset_y': array(189.0831, dtype=float32), 'qoffset_z': array(-933.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  206.66099548],
       [   0.        ,   -1.25      ,    0.        ,  189.08309937],
       [   0.        ,    0.        ,    5.        , -933.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  58], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0129_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  54,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.755, 0.755, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(200.794, dtype=float32), 'qoffset_y': array(192.9817, dtype=float32), 'qoffset_z': array(-953.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  200.79400635],
       [   0.        ,   -1.25      ,    0.        ,  192.98170471],
       [   0.        ,    0.        ,    5.        , -953.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  54], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0196_0_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140543700520080, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140543700519888, 'orig_size': (512, 512, 58), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[-7.40000010e-01,  0.00000000e+00,  0.00000000e+00,
         2.06660995e+02],
       [ 0.00000000e+00, -7.40000010e-01,  0.00000000e+00,
         1.89083099e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.33500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140543700520272, 'orig_size': (304, 304, 58), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140543700520208, 'orig_size': (304, 304, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140543700520080, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])}}, {'class': 'Spacingd', 'id': 140543700519888, 'orig_size': (512, 512, 54), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[-7.54999995e-01,  0.00000000e+00,  0.00000000e+00,
         2.00794006e+02],
       [ 0.00000000e+00, -7.54999995e-01,  0.00000000e+00,
         1.92981705e+02],
       [ 0.00000000e+00,  0.00000000e+00,  5.00000000e+00,
        -9.53500000e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140543700520272, 'orig_size': (310, 310, 54), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140543700520208, 'orig_size': (310, 310, 58), 'extra_info': {'center': [214, 96, 49]}}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 310, 310, 54)
Value range: (0.0, 1.0)
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5036 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.4450 
INFO:DataStats:input data information of the runtime error transform:
INFO:DataStats:image statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 1.0)
INFO:DataStats:label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:image_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  52,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(4, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.888, 0.888, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(10, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(1606, dtype=int32), 'glmin': array(-2048, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(240.766, dtype=float32), 'qoffset_y': array(227.0946, dtype=float32), 'qoffset_z': array(-852., dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -1.25      ,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  52], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0251_ct.nii.gz', 'patch_index': 0}
INFO:DataStats:label_meta_dict statistics:
Type: <class 'dict'>
Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  3, 512, 512,  73,   1,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(512, dtype=int16), 'bitpix': array(16, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.   , 0.741, 0.741, 5.   , 0.   , 0.   , 0.   , 0.   ],
      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(2, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(1, dtype=int16), 'sform_code': array(0, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(183.222, dtype=float32), 'qoffset_y': array(189.4729, dtype=float32), 'qoffset_z': array(-427.5, dtype=float32), 'srow_x': array([0., 0., 0., 0.], dtype=float32), 'srow_y': array([0., 0., 0., 0.], dtype=float32), 'srow_z': array([0., 0., 0., 0.], dtype=float32), 'affine': array([[  -1.25      ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -1.25      ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  73], dtype=int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train/volume-covid19-A-0255_seg.nii.gz', 'patch_index': 0}
INFO:DataStats:image_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140543700520080, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140543700519888, 'orig_size': (512, 512, 52), 'extra_info': {'meta_data_key': 'image_meta_dict', 'old_affine': array([[  -0.88800001,    0.        ,    0.        ,  240.76600647],
       [   0.        ,   -0.88800001,    0.        ,  227.09460449],
       [   0.        ,    0.        ,    5.        , -852.        ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'bilinear', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140543700520272, 'orig_size': (364, 364, 52), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140543700520208, 'orig_size': (364, 364, 58), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140543700519760, 'orig_size': (192, 192, 2), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140543700582800, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140543700583056, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140543700583184, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:label_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'Orientationd', 'id': 140543700520080, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]])}}, {'class': 'Spacingd', 'id': 140543700519888, 'orig_size': (512, 512, 73), 'extra_info': {'meta_data_key': 'label_meta_dict', 'old_affine': array([[  -0.741     ,    0.        ,    0.        ,  183.22200012],
       [   0.        ,   -0.741     ,    0.        ,  189.47290039],
       [   0.        ,    0.        ,    5.        , -427.5       ],
       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': False}}, {'class': 'SpatialPadd', 'id': 140543700520272, 'orig_size': (304, 304, 73), 'extra_info': {'mode': 'reflect'}}, {'class': 'RandCropByPosNegLabeld', 'id': 140543700520208, 'orig_size': (304, 304, 73), 'extra_info': {'center': [96, 180, 64]}}, {'class': 'RandAffined', 'id': 140543700519760, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'reflection'}, 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140543700582800, 'orig_size': (192, 192, 2), 'do_transforms': False}, {'class': 'RandFlipd', 'id': 140543700583056, 'orig_size': (192, 192, 2), 'do_transforms': True}, {'class': 'RandFlipd', 'id': 140543700583184, 'orig_size': (192, 192, 2), 'do_transforms': True}]
INFO:DataStats:image_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:label_1 statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 304, 304, 73)
Value range: (0.0, 1.0)
INFO:DataStats:synthetic_lesion statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 2)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_label statistics:
Type: <class 'numpy.ndarray'>
Shape: (1, 192, 192, 16)
Value range: (0.0, 0.0)
INFO:DataStats:synthetic_lesion_transforms statistics:
Type: <class 'list'>
Value: [{'class': 'RandAffined', 'id': 140543700519760, 'orig_size': (192, 192, 16), 'extra_info': {'affine': tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]], dtype=torch.float64), 'mode': 'bilinear', 'padding_mode': 'reflection'}, 'do_transforms': False}]
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4108 
ERROR:ignite.engine.engine.SupervisedTrainer:Current run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>

ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>

ERROR:ignite.engine.engine.SupervisedTrainer:Exception: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 850, in _run_once_on_dataset
    self._handle_exception(e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 467, in _handle_exception
    self._fire_event(Events.EXCEPTION_RAISED, e)
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 424, in _fire_event
    func(*first, *(event_args + others), **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/monai/handlers/stats_handler.py", line 145, in exception_raised
    raise e
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 801, in _run_once_on_dataset
    self.state.batch = next(self._dataloader_iter)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1183, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.7/dist-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in apply_transform
    return [transform(item) for item in data]
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 47, in <listcomp>
    return [transform(item) for item in data]
  File "run_net_aug.py", line 339, in __call__
    scan_slice = np.squeeze(d.get('image_1')[self.BATCH_SCAN,...,SLICE_I])
IndexError: index 54 is out of bounds for axis 3 with size 54

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 93, in __getitem__
    return self._transform(index)
  File "/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py", line 579, in _transform
    data = apply_transform(_transform, data)
  File "/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py", line 71, in apply_transform
    raise RuntimeError(f"applying transform {transform}") from e
RuntimeError: applying transform <__main__.TransCustom object at 0x7fd2e1584810>

INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5347 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.5083 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4883 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 4/9 -- train_loss: 1.4698 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 5/9 -- train_loss: 1.4808 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 6/9 -- train_loss: 1.4463 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 7/9 -- train_loss: 1.4445 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 8/9 -- train_loss: 1.4329 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 9/9 -- train_loss: 1.4016 
INFO:ignite.engine.engine.SupervisedEvaluator:Engine run resuming from iteration 0, epoch 0 until 1 epochs
INFO:ignite.engine.engine.SupervisedEvaluator:Got new best metric of val_mean_dice: 0.029906446114182472
INFO:ignite.engine.engine.SupervisedEvaluator:Epoch[1] Complete. Time taken: 00:00:16
INFO:ignite.engine.engine.SupervisedEvaluator:Engine run complete. Time taken: 00:00:16
INFO:ignite.engine.engine.SupervisedTrainer:Epoch[1] Complete. Time taken: 00:01:19
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 1/9 -- train_loss: 1.3734 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 2/9 -- train_loss: 1.3866 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 3/9 -- train_loss: 1.3750 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 4/9 -- train_loss: 1.3605 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 5/9 -- train_loss: 1.3573 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 6/9 -- train_loss: 1.3727 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 7/9 -- train_loss: 1.3402 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 8/9 -- train_loss: 1.3166 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 2/20, Iter: 9/9 -- train_loss: 1.3119 
INFO:ignite.engine.engine.SupervisedEvaluator:Engine run resuming from iteration 0, epoch 1 until 2 epochs
INFO:ignite.engine.engine.SupervisedEvaluator:Got new best metric of val_mean_dice: 0.035097669810056686
INFO:ignite.engine.engine.SupervisedEvaluator:Epoch[2] Complete. Time taken: 00:00:15
INFO:ignite.engine.engine.SupervisedEvaluator:Engine run complete. Time taken: 00:00:15
INFO:ignite.engine.engine.SupervisedTrainer:Epoch[2] Complete. Time taken: 00:01:16
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 3/20, Iter: 1/9 -- train_loss: 1.3039 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 3/20, Iter: 2/9 -- train_loss: 1.3011 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 3/20, Iter: 3/9 -- train_loss: 1.3014 
ERROR:ignite.engine.engine.SupervisedTrainer:Engine run is terminating due to exception: 
ERROR:ignite.engine.engine.SupervisedTrainer:Exception: 
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 745, in _internal_run
    time_taken = self._run_once_on_dataset()
  File "/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py", line 833, in _run_once_on_dataset
    self.state.output = self._process_function(self, self.state.batch)
  File "/usr/local/lib/python3.7/dist-packages/monai/engines/trainer.py", line 175, in _iteration
    self.scaler.step(self.optimizer)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 338, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py", line 284, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 1/9 -- train_loss: 1.5347 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 2/9 -- train_loss: 1.5083 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 3/9 -- train_loss: 1.4883 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 4/9 -- train_loss: 1.4698 
INFO:ignite.engine.engine.SupervisedTrainer:Epoch: 1/20, Iter: 5/9 -- train_loss: 1.4808 
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
INFO:root:training: image/label (20) folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:training: train 17 val 3, folder: /content/drive/MyDrive/Datasets/covid19/COVID-19-20_v2/Train
INFO:root:batch size 2
INFO:root:epochs 20, lr 0.0001, momentum 0.95
INFO:ignite.engine.engine.SupervisedTrainer:Engine run resuming from iteration 0, epoch 0 until 20 epochs
